{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import ollama\n",
    "from ollama import generate\n",
    "import spacy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import nltk\n",
    "from collections import deque\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'**NLTK**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Akshit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Akshit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JUST SCRAPPER NOT CRAWLER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_process_doj_website():\n",
    "    base_url = \"https://doj.gov.in/\"\n",
    "    queue = deque([base_url])\n",
    "    visited = set()\n",
    "    structured_content = []\n",
    "\n",
    "    while queue:\n",
    "        url = queue.popleft()\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            visited.add(url)\n",
    "\n",
    "            # Use NLP to identify headings and their associated content\n",
    "            page_content = {}\n",
    "            page_content['link'] = url\n",
    "            page_content['content'] = []\n",
    "\n",
    "            current_section = {\"heading\": None, \"text\": \"\"}\n",
    "\n",
    "            for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p']):\n",
    "                text = element.get_text(strip=True)\n",
    "                if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                    # Save previous section if it exists\n",
    "                    if current_section['heading'] or current_section['text']:\n",
    "                        page_content['content'].append(current_section)\n",
    "                    # Start a new section\n",
    "                    current_section = {\"heading\": text, \"text\": \"\"}\n",
    "                elif element.name == 'p':\n",
    "                    # Add paragraph text to the current section\n",
    "                    current_section['text'] += text + \" \"\n",
    "\n",
    "            # Save the last section\n",
    "            if current_section['heading'] or current_section['text']:\n",
    "                page_content['content'].append(current_section)\n",
    "\n",
    "            if page_content['content']:\n",
    "                structured_content.append(page_content)\n",
    "                print(f\"Scraped content from: {url}\")\n",
    "\n",
    "            # Find all links on the current page\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                full_url = urljoin(base_url, href)\n",
    "                \n",
    "                # Only add URLs from the same domain\n",
    "                if urlparse(full_url).netloc == urlparse(base_url).netloc and full_url not in visited:\n",
    "                    queue.append(full_url)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "    return structured_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVE FROM THE SCRAPPER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_extracted_data_to_json(content, filename=\"extracted_data.json\"):\n",
    "    if not content:\n",
    "        print(\"No content to save.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(content, file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Data successfully saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUTHENTICATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_user():\n",
    "    while True:\n",
    "        choice = input(\"Enter 'login' or 'signup': \").lower()\n",
    "        if choice == 'login':\n",
    "            username = input(\"Enter your username: \")\n",
    "            password = input(\"Enter your password: \")\n",
    "            if check_credentials(username, password):\n",
    "                return username\n",
    "            else:\n",
    "                print(\"Invalid credentials. Please try again.\")\n",
    "        elif choice == 'signup':\n",
    "            username = input(\"Choose a username: \")\n",
    "            password = input(\"Choose a password: \")\n",
    "            if create_user(username, password):\n",
    "                return username\n",
    "            else:\n",
    "                print(\"Username already exists. Please try again.\")\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter 'login' or 'signup'.\")\n",
    "\n",
    "def check_credentials(username, password):\n",
    "    if os.path.exists('users.json'):\n",
    "        with open('users.json', 'r') as f:\n",
    "            users = json.load(f)\n",
    "            return users.get(username) == password\n",
    "    return False\n",
    "\n",
    "def create_user(username, password):\n",
    "    if os.path.exists('users.json'):\n",
    "        with open('users.json', 'r') as f:\n",
    "            users = json.load(f)\n",
    "    else:\n",
    "        users = {}\n",
    "    \n",
    "    if username in users:\n",
    "        return False\n",
    "    \n",
    "    users[username] = password\n",
    "    with open('users.json', 'w') as f:\n",
    "        json.dump(users, f)\n",
    "    return True\n",
    "\n",
    "def save_conversation(username, question, answer):\n",
    "    if os.path.exists('conversations.json'):\n",
    "        with open('conversations.json', 'r') as f:\n",
    "            conversations = json.load(f)\n",
    "    else:\n",
    "        conversations = {}\n",
    "    \n",
    "    if username not in conversations:\n",
    "        conversations[username] = []\n",
    "    \n",
    "    conversations[username].append({\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'question': question,\n",
    "        'answer': answer\n",
    "    })\n",
    "    \n",
    "    with open('conversations.json', 'w') as f:\n",
    "        json.dump(conversations, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROCESS DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(filename=\"ollama_processed_data.json\"):\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = json.load(file)\n",
    "        print(f\"Data successfully loaded from {filename}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{filename} not found.\")\n",
    "        return None\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for page in content:\n",
    "        # Process the URL if it exists\n",
    "        if 'url' in page:\n",
    "            parsed_url = urlparse(page['url'])\n",
    "            page['processed_url'] = parsed_url.netloc + parsed_url.path\n",
    "        else:\n",
    "            page['processed_url'] = \"No URL provided\"\n",
    "\n",
    "        # Process the summary\n",
    "        if 'summary' in page and page['summary']:  # Add check for None\n",
    "            doc = nlp(page['summary'])\n",
    "            tokens = [lemmatizer.lemmatize(token.lemma_) for token in doc if not token.is_stop]\n",
    "            page['processed_summary'] = \" \".join(tokens)\n",
    "        else:\n",
    "            page['processed_summary'] = \"No summary provided\"\n",
    "        \n",
    "        # Process main topics\n",
    "        if 'main_topics' in page:\n",
    "            processed_topics = []\n",
    "            for topic in page['main_topics']:\n",
    "                if topic:  # Add check for None\n",
    "                    doc = nlp(topic)\n",
    "                    tokens = [lemmatizer.lemmatize(token.lemma_) for token in doc if not token.is_stop]\n",
    "                    processed_topics.append(\" \".join(tokens))\n",
    "                else:\n",
    "                    processed_topics.append(\"No topic provided\")\n",
    "            page['processed_topics'] = processed_topics\n",
    "        else:\n",
    "            page['processed_topics'] = []\n",
    "\n",
    "    try:\n",
    "        with open(\"processed_data.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(content, file, indent=4, ensure_ascii=False)\n",
    "        print(\"Processed data successfully saved to processed_data.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving processed data: {e}\")\n",
    "    \n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHATBOT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshit\\OneDrive\\Documents\\code\\python\\chatbot\\chatbot\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def generate_response(query, processed_content):\n",
    "    # Load the JSON data\n",
    "    with open('ollama_processed_data.json', 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    # Extract key terms from the query for matching\n",
    "    doc = nlp(query)\n",
    "    query_tokens = {token.lemma_ for token in doc if not token.is_stop}\n",
    "\n",
    "    # Search for relevant content in the JSON data\n",
    "    relevant_pages = []\n",
    "    for page in json_data:\n",
    "        title = page.get('title', '')\n",
    "        summary = page.get('summary', '')\n",
    "        main_topics = page.get('main_topics', [])\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        title_score = fuzz.token_set_ratio(query, title)\n",
    "        summary_score = fuzz.token_set_ratio(query, summary)\n",
    "        topics_score = max([fuzz.token_set_ratio(query, topic) for topic in main_topics]) if main_topics else 0\n",
    "        \n",
    "        # If any score is above a threshold, consider it relevant\n",
    "        if max(title_score, summary_score, topics_score) > 60:\n",
    "            relevant_pages.append(page)\n",
    "\n",
    "    # Sort relevant pages by similarity score\n",
    "    relevant_pages.sort(key=lambda x: fuzz.token_set_ratio(query, x['title']), reverse=True)\n",
    "\n",
    "    # Create the context from the relevant pages\n",
    "    context = \"\"\n",
    "    for page in relevant_pages[:3]:  # Limit to top 3 most relevant pages\n",
    "        title = page.get('title', 'No title available')\n",
    "        summary = page.get('summary', 'No summary available')\n",
    "        main_topics = page.get('main_topics', [])\n",
    "        url = page.get('url', 'No URL available')\n",
    "\n",
    "        context += f\"Title: {title}\\n\"\n",
    "        context += f\"Summary: {summary}\\n\"\n",
    "        context += \"Main Topics: \" + \", \".join(main_topics) + \"\\n\"\n",
    "        context += f\"URL: {url}\\n\\n\"\n",
    "    if os.path.exists('conversations.json'):\n",
    "        with open('conversations.json', 'r') as f:\n",
    "            conversations = json.load(f)\n",
    "            if username in conversations:\n",
    "                user_history = conversations[username][-5:]  # Get last 5 conversations\n",
    "                for conv in user_history:\n",
    "                    context += f\"Previous Q: {conv['question']}\\n\"\n",
    "                    context += f\"Previous A: {conv['answer']}\\n\\n\"\n",
    "\n",
    "    # Create a prompt using the collected context and query\n",
    "    prompt = f\"\"\"\n",
    "    You are an official, professional, and helpful chatbot for the Department of Justice website. Use the following context to answer the question:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Provide a clear, accurate, and detailed response. Include the relevant URL in the response.\n",
    "    Format your response in a friendly, conversational tone, as if you were a chatbot on the DOJ website.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the llama3.1 model to generate a response (replace with your actual model function)\n",
    "    response = generate(model=\"llama3.1\", prompt=prompt)\n",
    "    \n",
    "    response_text = response.get('response', 'I apologize, but I am unable to generate a response at the moment.')\n",
    "    \n",
    "    # Add a friendly opening and closing\n",
    "    opening = \"Hello! Thank you for your question. \"\n",
    "    closing = \" Is there anything else I can help you with?\"\n",
    "    \n",
    "    response_text = opening + response_text + closing\n",
    "    save_conversation(username, query, response_text)\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    username = authenticate_user()\n",
    "    print(f\"Welcome, {username}!\")\n",
    "\n",
    "    # Scrape and process the website (now includes crawling and NLP-based content structuring)\n",
    "    # doj_content = scrape_and_process_doj_website()\n",
    "    \n",
    "    # # Save extracted data to a JSON file\n",
    "    # save_extracted_data_to_json(doj_content)\n",
    "    \n",
    "    # Load and process data from the JSON file\n",
    "    processed_content = load_and_process_data()\n",
    "    \n",
    "    # Interactive Q&A loop\n",
    "    print(\"Ask questions about the Department of Justice website (type 'exit' to quit):\")\n",
    "    while True:\n",
    "        question = input(\"Q: \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        if processed_content:\n",
    "            answer = generate_response(question, processed_content)\n",
    "            print(f\"A: {answer}\\n\")\n",
    "        else:\n",
    "            print(\"No processed content available. Please check the data extraction and processing steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
